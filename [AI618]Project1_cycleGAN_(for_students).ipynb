{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2hRm9o68SLGo"
      },
      "source": [
        "# Individual Project: Unsupervised CT Denoising with CycleGAN\n",
        "This notebook is provided for unsupervised CT denoising task with cycleGAN.\n",
        "\n",
        "###Table of Contents\n",
        "\n",
        "I. Data: Google Drive, Dataloader\n",
        "\n",
        "II. Network: Generator / Discriminator\n",
        "\n",
        "III. Other functions\n",
        "\n",
        "IV. Training\n",
        "\n",
        "V. Test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJb_MPk-S5L_"
      },
      "source": [
        "## I. Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7h01yiT-S9Hq"
      },
      "source": [
        "### 1. Google Drive\n",
        "If you want to use data in your Google drive, you have to mount your google drive first.\n",
        "It makes possible to load data from the drive, save results in the drive, etc. \n",
        "If you try to mount Google Drive, a link for authentication code will be given. \n",
        "Then, follow the link, get your authentication code, and enter your code in the prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mqPWtOdEhk1W"
      },
      "outputs": [],
      "source": [
        "# Mount your Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_6VDOjbfiYe9"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load an example image\n",
        "img = np.load('/content/drive/My Drive/AAPM_data/train/full_dose/1.npy')\n",
        "\n",
        "# Change linear attenuation coefficient into HU values\n",
        "img = (img - 0.0192) / 0.0192 * 1000\n",
        "# Clip the CT image with [-1000, 1000] HU\n",
        "img = np.clip(img, -1000, 1000)\n",
        "\n",
        "# Plot the example image\n",
        "plt.imshow(img, cmap='gray')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AK0k3ee6WMss"
      },
      "source": [
        "### 2. Dataloader\n",
        "Before the training of cycleGAN, data preprocessing (e.g. cliping, normalization) is required.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v3G5AI-nonP-"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "from os import listdir\n",
        "from os.path import join\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "\n",
        "# CT dataset\n",
        "class CT_Dataset(Dataset):\n",
        "  def __init__(self, path, transform):\n",
        "    # Path of 'full_dose' and 'quarter_dose' folders\n",
        "    self.path_full = join(path, 'full_dose')\n",
        "    self.path_quarter = join(path, 'quarter_dose')\n",
        "    self.transform = transform\n",
        "\n",
        "    # File list of full dose data\n",
        "    self.file_full = list()\n",
        "    for file_name in sorted(listdir(self.path_full)):\n",
        "      self.file_full.append(file_name)\n",
        "    random.seed(0)\n",
        "    random.shuffle(self.file_full)\n",
        "    \n",
        "    # File list of quarter dose data\n",
        "    self.file_quarter = list()\n",
        "    for file_name in sorted(listdir(self.path_quarter)):\n",
        "      self.file_quarter.append(file_name)\n",
        "  \n",
        "  def __len__(self):\n",
        "    return min(len(self.file_full), len(self.file_quarter))\n",
        "  \n",
        "  def __getitem__(self, idx):\n",
        "    # Load full dose/quarter dose data\n",
        "    x_F = np.load(join(self.path_full, self.file_full[idx]))\n",
        "    x_Q = np.load(join(self.path_quarter, self.file_quarter[idx]))\n",
        "\n",
        "    # Convert to HU scale\n",
        "    x_F = (x_F - 0.0192) / 0.0192 * 1000\n",
        "    x_Q = (x_Q - 0.0192) / 0.0192 * 1000\n",
        "\n",
        "    # Normalize images\n",
        "    x_F[x_F < -1000] = -1000\n",
        "    x_Q[x_Q < -1000] = -1000\n",
        "\n",
        "    x_F = x_F / 4000\n",
        "    x_Q = x_Q / 4000\n",
        "\n",
        "    # Apply transform\n",
        "    x_F = self.transform(x_F)\n",
        "    x_Q = self.transform(x_Q)\n",
        "\n",
        "    file_name = self.file_quarter[idx]\n",
        "\n",
        "    return x_F, x_Q, file_name"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q2GnXM-eXM9M"
      },
      "source": [
        "Because the size of CT images is too large, we have to crop the images into small size patches for training.\n",
        "After training with patches, the whole size image will be used for the test."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HTWq-6WKXb64"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import torchvision\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "\n",
        "# Transform for the random crop\n",
        "class RandomCrop(object):\n",
        "  def __init__(self, patch_size):\n",
        "    self.patch_size = patch_size\n",
        "  \n",
        "  def __call__(self, img):\n",
        "    # Randomly crop the image into a patch with the size [self.patch_size, self.patch_size]\n",
        "    w, h = img.size(-1), img.size(-2)\n",
        "    i = random.randint(0, h - self.patch_size)\n",
        "    j = random.randint(0, w - self.patch_size)\n",
        "\n",
        "    return img[:, i:i + self.patch_size, j:j + self.patch_size]\n",
        "\n",
        "\n",
        "# Make dataloader for training/test\n",
        "def make_dataloader(path, batch_size):\n",
        "  # Path of 'train' and 'test' folders\n",
        "  path_train = join(path, 'train')\n",
        "  path_test = join(path, 'test')\n",
        "\n",
        "  # Transform for training data: convert to tensor, random horizontal/verical flip, random crop\n",
        "  # You can change transform if you want.\n",
        "  train_transform = torchvision.transforms.Compose([\n",
        "    torchvision.transforms.ToTensor(),\n",
        "    torchvision.transforms.RandomHorizontalFlip(p=0.5),\n",
        "    torchvision.transforms.RandomVerticalFlip(p=0.5),\n",
        "    RandomCrop(128)\n",
        "  ])\n",
        "\n",
        "  # Transform for test data: convert to tensor\n",
        "  test_transform = torchvision.transforms.Compose([\n",
        "    torchvision.transforms.ToTensor()\n",
        "  ])\n",
        "\n",
        "  # Generate CT dataset for training/test\n",
        "  train_dataset = CT_Dataset(path_train, train_transform)\n",
        "  test_dataset = CT_Dataset(path_test, test_transform)\n",
        "  \n",
        "  # Generate dataloader for training/test\n",
        "  train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True)\n",
        "  test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=0, pin_memory=True)\n",
        "\n",
        "  return train_dataloader, test_dataloader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Z2QlnrKbVr8"
      },
      "source": [
        "## II. Network: Generator / Discriminator\n",
        "You have to implement basic blocks for generator/discriminator.\n",
        "Refer to the 'ConvBlock', and make your own blocks for building generators/discriminators."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NI1fbN6R-qBl"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "# Example convolution block. You don't have to use this block.\n",
        "# (Convolution, Batch normalization, ReLU) x 2\n",
        "class ConvBlock(nn.Module):\n",
        "  def __init__(self, in_channels, mid_channels, out_channels):\n",
        "    super(ConvBlock, self).__init__()\n",
        "    self.conv = nn.Sequential(\n",
        "        nn.Conv2d(in_channels, mid_channels, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "        nn.BatchNorm2d(mid_channels, affine=True, track_running_stats=True),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(mid_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "        nn.BatchNorm2d(out_channels, affine=True, track_running_stats=True),\n",
        "        nn.ReLU()\n",
        "    )\n",
        "  \n",
        "  def forward(self, x):\n",
        "    out = self.conv(x)\n",
        "    return out\n",
        "  \n",
        "# You can implement other building blocks for making generator and discriminator.\n",
        "######################\n",
        "# Your code\n",
        "######################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y58QwHbicc_f"
      },
      "source": [
        "### 1. Generator\n",
        "Now, implement the generator and discriminator below.\n",
        "You are free to choose the structure of the generator (e.g. U-Net, ResNet), but it should contain residual path."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GAMI3pbWbhE9"
      },
      "outputs": [],
      "source": [
        "# Generator\n",
        "class Generator(nn.Module):\n",
        "  def __init__(self, in_channels, out_channels, ngf):\n",
        "    super(Generator, self).__init__()\n",
        "    # in_channels: the number of channels of the input\n",
        "    # out_channels: the number of channels of the output\n",
        "    # ngf: the number of convolution filters of the first layer\n",
        "    ######################\n",
        "    # Your code\n",
        "    ######################\n",
        "  \n",
        "  def forward(self, x):\n",
        "    ######################\n",
        "    # Your code\n",
        "    ######################\n",
        "\n",
        "    # Residual path: final output = output + input\n",
        "    ######################\n",
        "    # Your code\n",
        "    ######################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndKmNFTse53O"
      },
      "source": [
        "### 2. Discriminator\n",
        "You have to construct PatchGAN structure for the discriminator as shown in PPT slide."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xqza1MTYe-70"
      },
      "outputs": [],
      "source": [
        "# Discriminator (PatchGAN)\n",
        "class Discriminator(nn.Module):\n",
        "  def __init__(self, in_channels, ndf):\n",
        "    super(Discriminator, self).__init__()\n",
        "    # in_channels: the number of channels of the input\n",
        "    # ndf: the number of convolution filters of the first layer\n",
        "    ######################\n",
        "    # Your code\n",
        "    ######################\n",
        "  \n",
        "  def forward(self, x):\n",
        "    ######################\n",
        "    # Your code\n",
        "    ######################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q2hqiJnkemP3"
      },
      "source": [
        "## III. Other functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RDG_8KraXLM7"
      },
      "outputs": [],
      "source": [
        "from torch.nn import init\n",
        "\n",
        "\n",
        "# initialize parameters of neural networks\n",
        "def init_weights(net):\n",
        "  def init_func(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if hasattr(m, 'weight') and (classname.find('Conv') != -1 or classname.find('Linear') != -1):\n",
        "      init.normal_(m.weight.data, 0.0, 0.02)\n",
        "      if hasattr(m, 'bias') and m.bias is not None:\n",
        "        init.constant_(m.bias.data, 0.0)\n",
        "    elif classname.find('BatchNorm2d') != -1:\n",
        "      init.normal_(m.weight.data, 1.0, 0.02)\n",
        "      init.constant_(m.bias.data, 0.0)\n",
        "    \n",
        "  print('Initialize network.')\n",
        "  net.apply(init_func)\n",
        "\n",
        "\n",
        "# Calculate average loss during one epoch\n",
        "class Mean:\n",
        "  def __init__(self):\n",
        "    self.numel = 0\n",
        "    self.mean = 0\n",
        "  \n",
        "  def __call__(self, val):\n",
        "    self.mean = self.mean * (self.numel / (self.numel + 1)) + val / (self.numel + 1)\n",
        "    self.numel += 1\n",
        "  \n",
        "  def result(self):\n",
        "    return self.mean\n",
        "\n",
        "\n",
        "# Show input and output images during training\n",
        "def show_imgs(imgs):\n",
        "  FQF = np.concatenate(imgs[:3], axis=2)\n",
        "  QFQ = np.concatenate(imgs[3:], axis=2)\n",
        "  img_array = np.squeeze(np.concatenate([FQF, QFQ], axis=1))\n",
        "\n",
        "  img_array = img_array * 4000\n",
        "  img_array = np.clip(img_array, -1000, 1000)\n",
        "\n",
        "  plt.imshow(img_array, cmap='gray')\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "# Set 'requires_grad' of the networks\n",
        "def set_requires_grad(nets, requires_grad=False):\n",
        "  if not isinstance(nets, list):\n",
        "    nets = [nets]\n",
        "  for net in nets:\n",
        "    if net is not None:\n",
        "      for param in net.parameters():\n",
        "        param.requires_grad = requires_grad"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xVWlp65Sfs6P"
      },
      "source": [
        "## IV. Training\n",
        "Before training the network, some hyperparameters should be defined as follows.\n",
        "You can change the value of hyperparameters if you want."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G63jcZgQf66Z"
      },
      "outputs": [],
      "source": [
        "from os import makedirs\n",
        "from os.path import isdir\n",
        "\n",
        "# Hyperparameters\n",
        "# You can change hyperparameters to find your best performance in your architecture.\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "batch_size = 16\n",
        "lambda_cycle = 10\n",
        "lambda_iden = 5\n",
        "beta1 = 0.5\n",
        "beta2 = 0.999\n",
        "num_epoch = 100\n",
        "lr = 2e-4\n",
        "\n",
        "# Path for saving the checkpoint\n",
        "path_checkpoint = '/content/drive/My Drive/CT_denoising'\n",
        "if not isdir(path_checkpoint):\n",
        "  makedirs(path_checkpoint)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TaxBVSlbiNIq"
      },
      "source": [
        "```model_name``` is the name of the model, and it will be used for saving the model.\n",
        "If you want to continue the training from the last checkpoint, set ```model_name``` as the name of the saved model.\n",
        "However, if you want to train a new model, you have to change ```model_name```."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dz0WHWauLdZi"
      },
      "outputs": [],
      "source": [
        "model_name = 'cyclegan_v1'\n",
        "\n",
        "# Path for saving results\n",
        "path_result = join(path_checkpoint, model_name)\n",
        "if not isdir(path_result):\n",
        "  makedirs(path_result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hiIRV1O1gL90"
      },
      "source": [
        "Next, make dataloaders, networks, optimizers, and define loss functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z1e_uJN1PCf-"
      },
      "outputs": [],
      "source": [
        "import itertools\n",
        "\n",
        "# path of dataset (change the path according to your setting)\n",
        "path_data = '/content/drive/My Drive/AAPM_data'\n",
        "\n",
        "# Make dataloaders\n",
        "train_dataloader, test_dataloader = make_dataloader(path_data, batch_size)\n",
        "\n",
        "# Make generators (G_F2Q: full to quarter / G_Q2F: quarter to full)\n",
        "####################\n",
        "G_F2Q = # your code\n",
        "G_Q2F = # your code\n",
        "####################\n",
        "\n",
        "# Make discriminators (D_F: distinguish real/fake full dose images / D_Q: distinguish real/fake quarter dose images)\n",
        "####################\n",
        "D_F = # your code\n",
        "D_Q = # your code\n",
        "####################\n",
        "\n",
        "# Make optimizers\n",
        "G_optim = torch.optim.Adam(itertools.chain(G_F2Q.parameters(), G_Q2F.parameters()), lr, betas=(beta1, beta2))\n",
        "D_optim = torch.optim.Adam(itertools.chain(D_F.parameters(), D_Q.parameters()), lr, betas=(beta1, beta2))\n",
        "\n",
        "# Define loss functions\n",
        "adv_loss = nn.MSELoss()\n",
        "cycle_loss = nn.L1Loss()\n",
        "iden_loss = nn.L1Loss()\n",
        "\n",
        "# Loss functions\n",
        "loss_name = ['G_adv_loss_F',\n",
        "             'G_adv_loss_Q',\n",
        "             'G_cycle_loss_F',\n",
        "             'G_cycle_loss_Q',\n",
        "             'G_iden_loss_F',\n",
        "             'G_iden_loss_Q',\n",
        "             'D_adv_loss_F',\n",
        "             'D_adv_loss_Q']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nS2gNrsiT6vc"
      },
      "outputs": [],
      "source": [
        "from os.path import isfile\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "\n",
        "# Load the last checkpoint if it exists\n",
        "if isfile(join(path_checkpoint, model_name + '.pth')):\n",
        "  checkpoint = torch.load(join(path_checkpoint, model_name + '.pth'))\n",
        "  G_F2Q.load_state_dict(checkpoint['G_F2Q_state_dict'])\n",
        "  G_Q2F.load_state_dict(checkpoint['G_Q2F_state_dict'])\n",
        "  D_F.load_state_dict(checkpoint['D_F_state_dict'])\n",
        "  D_Q.load_state_dict(checkpoint['D_Q_state_dict'])\n",
        "  G_optim.load_state_dict(checkpoint['G_optim_state_dict'])\n",
        "  D_optim.load_state_dict(checkpoint['D_optim_state_dict'])\n",
        "  trained_epoch = checkpoint['epoch']\n",
        "  losses_list = {name: torch.load(join(path_result, name + '.npy')) for name in loss_name}\n",
        "  print('Start from save model - ' + str(trained_epoch))\n",
        "# If the checkpoint does not exist, start the training with random initialized model\n",
        "else:\n",
        "  init_weights(G_F2Q)\n",
        "  init_weights(G_Q2F)\n",
        "  init_weights(D_F)\n",
        "  init_weights(D_Q)\n",
        "  trained_epoch = 0\n",
        "  losses_list = {name: list() for name in loss_name}\n",
        "  print('Start from random initialized model')\n",
        "\n",
        "for epoch in tqdm(range(trained_epoch, num_epoch), desc='Epoch', total=num_epoch, initial=trained_epoch):\n",
        "  losses = {name: Mean() for name in loss_name}\n",
        "\n",
        "  for x_F, x_Q, _ in tqdm(train_dataloader, desc='Step'):\n",
        "    x_F = x_F.to(device)\n",
        "    x_Q = x_Q.to(device)\n",
        "\n",
        "    # Set 'requires_grad' of the discriminators as 'False'\n",
        "    ####################\n",
        "    # Your code\n",
        "    ####################\n",
        "\n",
        "    x_FQ = # your code\n",
        "    x_QF = # your code\n",
        "    x_QFQ = # your code\n",
        "    x_FQF = # your code\n",
        "    x_QQ = # your code\n",
        "    x_FF = # your code\n",
        "\n",
        "    G_adv_loss_F = # your code\n",
        "    G_adv_loss_Q = # your code\n",
        "    G_cycle_loss_F = # your code\n",
        "    G_cycle_loss_Q = # your code\n",
        "    G_iden_loss_F = # your code\n",
        "    G_iden_loss_Q = # your code\n",
        "    G_adv_loss = G_adv_loss_F + G_adv_loss_Q\n",
        "    G_cycle_loss = G_cycle_loss_F + G_cycle_loss_Q\n",
        "    G_iden_loss = G_iden_loss_F + G_iden_loss_Q\n",
        "    G_total_loss = G_adv_loss_F + G_adv_loss_Q + lambda_cycle * (G_cycle_loss) + lambda_iden * (G_iden_loss)\n",
        "\n",
        "    G_optim.zero_grad()\n",
        "    G_total_loss.backward()\n",
        "    G_optim.step()\n",
        "    \n",
        "    # Set 'requires_grad' of the discriminators as 'True'\n",
        "    ####################\n",
        "    # Your code\n",
        "    ####################\n",
        "\n",
        "    # You have to detach the outputs of the generators in below codes\n",
        "    D_adv_loss_F = # your code\n",
        "    D_adv_loss_Q = # your code\n",
        "    D_total_loss_F = D_adv_loss_F / 2.0\n",
        "    D_total_loss_Q = D_adv_loss_Q / 2.0\n",
        "\n",
        "    D_optim.zero_grad()\n",
        "    D_total_loss_F.backward()\n",
        "    D_total_loss_Q.backward()\n",
        "    D_optim.step()\n",
        "\n",
        "    # Calculate the average loss during one epoch\n",
        "    losses['G_adv_loss_F'](G_adv_loss_F.detach())\n",
        "    losses['G_adv_loss_Q'](G_adv_loss_Q.detach())\n",
        "    losses['G_cycle_loss_F'](G_cycle_loss_F.detach())\n",
        "    losses['G_cycle_loss_Q'](G_cycle_loss_Q.detach())\n",
        "    losses['G_iden_loss_F'](G_iden_loss_F.detach())\n",
        "    losses['G_iden_loss_Q'](G_iden_loss_Q.detach())\n",
        "    losses['D_adv_loss_F'](D_adv_loss_F.detach())\n",
        "    losses['D_adv_loss_Q'](D_adv_loss_Q.detach())\n",
        "  \n",
        "  for name in loss_name:\n",
        "    losses_list[name].append(losses[name].result())\n",
        "  \n",
        "  # Save the trained model and list of losses\n",
        "  torch.save({'epoch': epoch + 1, 'G_F2Q_state_dict': G_F2Q.state_dict(), 'G_Q2F_state_dict': G_Q2F.state_dict(),\n",
        "              'D_F_state_dict': D_F.state_dict(), 'D_Q_state_dict': D_Q.state_dict(),\n",
        "              'G_optim_state_dict': G_optim.state_dict(), 'D_optim_state_dict': D_optim.state_dict()}, join(path_checkpoint, model_name + '.pth'))\n",
        "  for name in loss_name:\n",
        "    torch.save(losses_list[name], join(path_result, name + '.npy'))\n",
        "  \n",
        "  # Plot input/output images every 10 epochs\n",
        "  if (epoch + 1) % 10 == 0:\n",
        "    imgs = [x_F[0].detach().cpu().numpy(), x_FQ[0].detach().cpu().numpy(), x_FQF[0].detach().cpu().numpy(),\n",
        "            x_Q[0].detach().cpu().numpy(), x_QF[0].detach().cpu().numpy(), x_QFQ[0].detach().cpu().numpy()]\n",
        "    show_imgs(imgs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5GbriABk6EG4"
      },
      "source": [
        "## V. Test\n",
        "Last, you have to verify the performance of your network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5WF5Lsovgf9E"
      },
      "outputs": [],
      "source": [
        "from tqdm.auto import tqdm\n",
        "\n",
        "# Load the last checkpoint\n",
        "checkpoint = torch.load(join(path_checkpoint, model_name + '.pth'))\n",
        "G_Q2F.load_state_dict(checkpoint['G_Q2F_state_dict'])\n",
        "G_Q2F.eval()\n",
        "\n",
        "# Test and save\n",
        "with torch.no_grad():\n",
        "  for _, x_Q, file_name in tqdm(test_dataloader):\n",
        "    x_Q = x_Q.to(device)\n",
        "    x_QF = G_Q2F(x_Q)[0].detach().cpu().numpy()\n",
        "    x_QF = x_QF * 4000\n",
        "\n",
        "    np.save(join(path_result, file_name[0]), x_QF)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R3qdb21fGOSi"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot loss graph (adversarial loss)\n",
        "x_axis = np.arange(1, num_epoch + 1)\n",
        "plt.figure(1)\n",
        "for name in ['G_adv_loss_F', 'G_adv_loss_Q', 'D_adv_loss_F', 'D_adv_loss_Q']:\n",
        "  loss_arr = torch.load(join(path_result, name + '.npy'))\n",
        "  plt.plot(x_axis, loss_arr, label=name)\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.savefig(join(path_result, 'loss_curve_1.png'))\n",
        "plt.show()\n",
        "\n",
        "# Plot loss graph (cycle consistency loss, identity loss)\n",
        "plt.figure(2)\n",
        "for name in ['G_cycle_loss_F', 'G_cycle_loss_Q', 'G_identity_loss_F', 'G_identity_loss_Q']:\n",
        "  loss_arr = torch.load(join(path_result, name + '.npy'))\n",
        "  plt.plot(x_axis, loss_arr, label=name)\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.savefig(join(path_result, 'loss_curve_2.png'))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jgMYN7B_nsdu"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Example of result\n",
        "path_quarter = join(path_data, 'test/quarter_dose/100.npy')\n",
        "path_full = join(path_data, 'test/full_dose/100.npy')\n",
        "path_output = join(path_result, '100.npy')\n",
        "\n",
        "quarter = np.load(path_quarter)\n",
        "full = np.load(path_full)\n",
        "output = np.load(path_output)\n",
        "\n",
        "quarter = (quarter - 0.0192) / 0.0192 * 1000\n",
        "full = (full - 0.0192) / 0.0192 * 1000\n",
        "\n",
        "quarter = np.clip(quarter, -1000, 1000)\n",
        "full = np.clip(full, -1000, 1000)\n",
        "output = np.clip(output, -1000, 1000)\n",
        "\n",
        "plt.imshow(quarter, cmap='gray')\n",
        "plt.show()\n",
        "plt.imshow(full, cmap='gray')\n",
        "plt.show()\n",
        "plt.imshow(output, cmap='gray')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N5Y0vOhTVlTR"
      },
      "outputs": [],
      "source": [
        "from skimage.metrics import peak_signal_noise_ratio, structural_similarity\n",
        "\n",
        "# Functions for caculating PSNR, SSIM\n",
        "def psnr(A, ref):\n",
        "  ref[ref < -1000] = -1000\n",
        "  A[A < -1000] = -1000\n",
        "  val_min = -1000\n",
        "  val_max = np.amax(ref)\n",
        "  ref = (ref - val_min) / (val_max - val_min)\n",
        "  A = (A - val_min) / (val_max - val_min)\n",
        "  out = peak_signal_noise_ratio(ref, A)\n",
        "  return out\n",
        "\n",
        "def ssim(A, ref):\n",
        "  ref[ref < -1000] = -1000\n",
        "  A[A < -1000] = -1000\n",
        "  val_min = -1000\n",
        "  val_max = np.amax(ref)\n",
        "  ref = (ref - val_min) / (val_max - val_min)\n",
        "  A = (A - val_min) / (val_max - val_min)\n",
        "  out = structural_similarity(ref, A, data_range=2)\n",
        "  return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RbqJr-7fVqNl"
      },
      "outputs": [],
      "source": [
        "# How to use functions 'psnr' and 'ssim'\n",
        "path_quarter = join(path_data, 'test/quarter_dose/100.npy')\n",
        "path_full = join(path_data, 'test/full_dose/100.npy')\n",
        "path_output = join(path_result, '100.npy')\n",
        "\n",
        "quarter = np.load(path_quarter)\n",
        "full = np.load(path_full)\n",
        "output = np.load(path_output)\n",
        "\n",
        "quarter = (quarter - 0.0192) / 0.0192 * 1000\n",
        "full = (full - 0.0192) / 0.0192 * 1000\n",
        "\n",
        "print(psnr(quarter, full))\n",
        "print(ssim(quarter, full))\n",
        "print(psnr(output, full))\n",
        "print(ssim(output, full))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "[BiS800]Project2_cycleGAN (for students).ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
